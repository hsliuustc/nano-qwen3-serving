# è°ƒç”¨å…³ç³»å›¾

## ğŸ”„ è¯·æ±‚å¤„ç†æµç¨‹

### 1. **HTTP API è¯·æ±‚æµç¨‹**

```
Client Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server â”‚  â† /v1/chat/completions
â”‚  (openai_server)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI Service  â”‚  â† OpenAICompatibleService
â”‚ (openai_service)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLM        â”‚  â† generate_single()
â”‚   (core/llm)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLMEngine     â”‚  â† generate()
â”‚  (core/engine)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ModelRunner    â”‚  â† run_model()
â”‚ (core/model_runner)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DeviceManager   â”‚  â† optimize_for_device()
â”‚(core/device_manager)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **å¼‚æ­¥è¯·æ±‚æµç¨‹**

```
Async Client Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AsyncLLM      â”‚  â† generate()
â”‚ (async/async_llm)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AsyncLLMEngine  â”‚  â† generate_async()
â”‚(async/async_engine)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLMEngine     â”‚  â† generate() (sync)
â”‚  (core/engine)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ModelRunner    â”‚  â† run_model()
â”‚ (core/model_runner)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ è¯¦ç»†è°ƒç”¨å…³ç³»

### **API Layer è°ƒç”¨å…³ç³»**

#### FastAPI Server â†’ OpenAI Service
```python
# openai_server.py
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    from .openai_service import service
    return await service.chat_completions(request)
```

#### OpenAI Service â†’ LLM
```python
# openai_service.py
async def chat_completions(self, request: ChatCompletionRequest):
    result = await loop.run_in_executor(
        None,
        lambda: self.llm.generate_single(prompt, sampling_params)
    )
```

### **Core Layer è°ƒç”¨å…³ç³»**

#### LLM â†’ LLMEngine
```python
# core/llm.py
def generate(self, prompts, sampling_params=None, priority=RequestPriority.NORMAL):
    results = self.engine.generate(
        prompts=prompts,
        sampling_params=sampling_params,
        priority=priority
    )
```

#### LLMEngine â†’ ModelRunner
```python
# core/engine.py
def _process_request(self, request_id: int):
    # Get input tokens
    input_tokens = self.model_runner.tokenize(prompt)
    
    # Run model inference
    outputs = self.model_runner.run_model(
        input_ids=input_tokens,
        past_key_values=past_key_values
    )
```

### **Execution Layer è°ƒç”¨å…³ç³»**

#### ModelRunner â†’ DeviceManager
```python
# core/model_runner.py
def __init__(self, model_name, device="auto", dtype=None):
    self.device_manager = DeviceManager(device)
    self.device = self.device_manager.device
    
    # Apply device-specific optimizations
    if self.model is not None:
        self.model = self.device_manager.optimize_for_device(self.model)
```

#### LLMEngine â†’ Scheduler
```python
# core/engine.py
def generate(self, prompts, sampling_params=None, priority=RequestPriority.NORMAL):
    # Add requests to scheduler
    request_ids = []
    for prompt in prompts:
        request_id = self.scheduler.add_request(prompt, sampling_params, priority)
        request_ids.append(request_id)
```

#### LLMEngine â†’ BlockManager
```python
# core/engine.py
def _process_request(self, request_id: int):
    # Allocate memory blocks
    block_indices = self.block_manager.allocate_blocks(sequence_length)
    
    # Free blocks when done
    self.block_manager.free_blocks(block_indices)
```

## ğŸ¯ å…³é”®æ¥å£

### **1. åŒæ­¥æ¥å£**
```python
# é«˜çº§æ¥å£
llm = LLM(model_name="Qwen/Qwen3-0.6B", device="auto")
result = llm.generate("Hello, world!")

# å¼•æ“æ¥å£
engine = LLMEngine(model_name="Qwen/Qwen3-0.6B", device="auto")
results = engine.generate(["Hello", "World"])

# æ¨¡å‹æ¥å£
runner = ModelRunner(model_name="Qwen/Qwen3-0.6B", device="auto")
outputs = runner.run_model(input_tokens)
```

### **2. å¼‚æ­¥æ¥å£**
```python
# å¼‚æ­¥é«˜çº§æ¥å£
async with AsyncLLM() as llm:
    result = await llm.generate("Hello, world!")

# å¼‚æ­¥å¼•æ“æ¥å£
async_engine = AsyncLLMEngine()
results = await async_engine.generate_async(["Hello", "World"])
```

### **3. HTTP APIæ¥å£**
```python
# OpenAIå…¼å®¹æ¥å£
POST /v1/chat/completions
{
    "model": "qwen3-0.6b",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false
}
```

## ğŸ”§ ç»„ä»¶ä¾èµ–å…³ç³»

### **åˆå§‹åŒ–ä¾èµ–**
```
LLM
â”œâ”€â”€ LLMEngine
â”‚   â”œâ”€â”€ ModelRunner
â”‚   â”‚   â””â”€â”€ DeviceManager
â”‚   â”œâ”€â”€ Scheduler
â”‚   â””â”€â”€ BlockManager
â””â”€â”€ SamplingParams

AsyncLLM
â””â”€â”€ AsyncLLMEngine
    â””â”€â”€ LLMEngine (åŒä¸Š)

OpenAICompatibleService
â””â”€â”€ LLM (åŒä¸Š)
```

### **è¿è¡Œæ—¶ä¾èµ–**
```
Request Flow:
FastAPI â†’ OpenAI Service â†’ LLM â†’ LLMEngine â†’ ModelRunner â†’ DeviceManager

Async Flow:
AsyncLLM â†’ AsyncLLMEngine â†’ LLMEngine â†’ ModelRunner â†’ DeviceManager

Memory Flow:
LLMEngine â†’ BlockManager â†’ Memory Allocation
LLMEngine â†’ Scheduler â†’ Request Queue
```

## ğŸ“Š æ€§èƒ½å…³é”®è·¯å¾„

### **æ¨ç†è·¯å¾„**
1. **Tokenization**: `ModelRunner.tokenize()`
2. **Model Forward**: `ModelRunner.run_model()`
3. **Sampling**: `ModelRunner._sample_token()`
4. **Detokenization**: `ModelRunner.detokenize()`

### **æ‰¹å¤„ç†è·¯å¾„**
1. **Request Collection**: `Scheduler.get_ready_requests()`
2. **Batch Formation**: `LLMEngine._process_batch()`
3. **Batch Inference**: `ModelRunner.run_model_batch()`
4. **Result Distribution**: `LLMEngine._distribute_results()`

### **å†…å­˜ç®¡ç†è·¯å¾„**
1. **Block Allocation**: `BlockManager.allocate_blocks()`
2. **KV Cache Management**: `ModelRunner` with `past_key_values`
3. **Block Deallocation**: `BlockManager.free_blocks()`

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†æµç¨‹

### **è®¾å¤‡é”™è¯¯**
```
DeviceManager.detect_device() â†’ å›é€€åˆ°CPU
ModelRunner._load_model() â†’ å¼‚å¸¸å¤„ç†
```

### **å†…å­˜é”™è¯¯**
```
BlockManager.allocate_blocks() â†’ å†…å­˜ä¸è¶³å¤„ç†
LLMEngine._process_request() â†’ è¯·æ±‚å¤±è´¥å¤„ç†
```

### **æ¨¡å‹é”™è¯¯**
```
ModelRunner.run_model() â†’ æ¨ç†å¼‚å¸¸å¤„ç†
LLMEngine.generate() â†’ ç”Ÿæˆå¤±è´¥å¤„ç†
```

## ğŸ”„ å¹¶å‘å¤„ç†

### **å¼‚æ­¥å¹¶å‘**
- **AsyncLLM**: å¤šä¸ªå¹¶å‘è¯·æ±‚
- **AsyncLLMEngine**: å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
- **Worker Pool**: åå°å¤„ç†çº¿ç¨‹

### **æ‰¹å¤„ç†å¹¶å‘**
- **Scheduler**: è¯·æ±‚é˜Ÿåˆ—ç®¡ç†
- **LLMEngine**: æ‰¹å¤„ç†åè°ƒ
- **ModelRunner**: æ‰¹é‡æ¨ç†

### **å†…å­˜å¹¶å‘**
- **BlockManager**: çº¿ç¨‹å®‰å…¨çš„å†…å­˜åˆ†é…
- **KV Cache**: å¹¶å‘è®¿é—®æ§åˆ¶
- **Device Management**: è®¾å¤‡çŠ¶æ€åŒæ­¥ 